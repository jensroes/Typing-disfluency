---
title             : "Modelling disfluencies in copy-typing"
shorttitle        : "Modelling disfluencies in copy-typing"

author: 
  - name          : "Jens Roeser"
    affiliation   : "1"
    address       : "50 Shakespeare St, Nottingham NG1 4FQ"
    corresponding : yes    # Define only one corresponding author
    email         : "jens.roeser@ntu.ac.uk"
  - name          : "Sven De Maeyer"
    affiliation   : "2"
  - name          : "Mark Torrance"
    affiliation   : "1"
  - name          : "Luuk Van Waes"
    affiliation   : "3"
  - name          : "MariÃ«lle Leijten"
    affiliation   : "3"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Nottingham Trent University, United Kingdom"

  - id            : "2"
    institution   : "Faculty of Social Sciences, University of Antwerp, Belgium"

  - id            : "3"
    institution   : "Department of Management, University of Antwerp, Belgium"


abstract: |
   The analysis of keystroke timing data typically involves the calculation of aggregates such as the mean inter-keystroke interval or pause frequencies. There are two fundamental problems with this: first, parametric descriptives are biased estimates for non-normal distributed data and neglects information associated with variance; second, pauses and related estimtes (e.g. bursts) require a minimum threshold value which is in principle arbitrary. We implemented a series of Bayesian models that aimed to address both issues by (a) providing reliable tpying estimates and (b) statistically detecting process disfluencies. We tested these models on a random sample of 100 participants (age 18-25) from the Dutch copy-task corpus. Our results show that disfluent typing can be determined statistically using a finite mixture process. This model characterises the typing task as a combination of fluent and disfluent typing associated with parameters that capture the magnitude and the probability of disfluencies. In other words, mixture models constitute a principle method of detecting disfluencies during keyboard typing.


keywords: "Copy-task; keystroke modeling; autoregression; mixture models; Bayesian inference"


bibliography      : ["ref.bib"]


documentclass     : "apa7"
classoption       : "jou"
output            : 
  papaja::apa6_pdf:
    keep_tex: TRUE


figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no

header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{float}
  - \usepackage{colortbl}
  - \usepackage{threeparttable}
  - \usepackage[normalem]{ulem}
  - \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE,
                      cache.extra = R.version
                      )
```


```{r load_packages}
source("../functions/functions.R")
source("../functions/get_data.R")
library(magrittr)
options(kableExtra.auto_format = FALSE)
library(kableExtra)
library(tidyverse)
library(wesanderson)
library(gridExtra)
library(knitr)
library(brms)
library(citr)
library(grid)
library(rmdfiltr)
library(gtable)
library(papaja)
library(ggforce)
library(ggthemes)
library(ggExtra)
dev.args = list(pdf = list(type = "cairo"))
knitr::opts_chunk$set(dev = "cairo_pdf")
```


# Introduction

Writing research has made extensive use of keystroke-logging to capture typing process data. In particular disfluencies (loosly defined as relatively long intervals between subsequent keystrokes) are interesting as these are indicators of processing demands [@chukharev2019combined] which may arise on various levels of mental representation [@olive2014toward]; for example, when preplanning syntactic dependencies [@roeser2019advance], retrieving a lexical entry for a word or its spelling [@torrance2016adolescent]. At present there is no principled way of determining which keystroke intervals constitute a process disfluency. In this paper we present a series of statistical models that aimed at capturing the typing process and in particular process disfluencies.

Keystroke logs provide very rich information about the typing process. To reduce the complexity of the data, researchers have often performed analysis on the means, medians, standard deviations (SD) etc. of inter-keystroke intervals (the duration between two consecutive keystrokes), number of pauses or pause durations, within-word keystroke intervals and many other variables [for an overview see @conijn2019understanding]. @conijn2019understanding suggested that different aggregates are sensitive to processing difficulty that arises on different levels of mental representation. However, using data aggregates comes with a number of problems. First, a feature such as pause duration or number of puases requires a definition what consitutes a pause. Researchs have often used a conventional but arbitrary threshold of 2 secs to define pauses [@alves2015progress; @chukharev2014pauses; @connelly2012predicting; @leijten2013keystroke]. Second, aggregating data artificially reduces the information in the data requires trimming criteria [@hoaglin1987fine] which remove certain data as outliers that might be valuable information about the writing processs. For example, data aggregation disregards timecourse variation and autocorrelation between consecutive keystrokes [@eltahir2004dynamic]. Third, parametric aggrgates such the mean and the SD function assume that, for the aggregated data point to be representative for the sample, keystroke intervals must come from a normal distribution. This is not the case as keystroke intervals are zero-bound and therefore right-skewed.^[In fact, the minimum size of keystroke intervals is determined by the time it takes to plan and execute the motor programm.] Hence, aggregates are biased estimates of the typing process and therefore may lead to incorrect inference.

While the former problems are statistical problems they are interlinked with a conceptual issue. Statistical models make assumptions about the nature of the data generating process. For example, most standard statistical models assume that the data come from a single process that generates normal distributed data. We already addressed that keystroke data are not normal distributed. Another problem is that the underlying mental process that generates keystroke data involves cognitive processing on different levels mental representation. Process diffculty on any of these levels inhibits the acivation flow into lower levels of representation which then creates disfluencies [see @christiansen2016now;@olive2014toward]. For example, copying a sequence of letters involves visual encoding of the sequence of letters, buffering n-bigrams in memory, identifying the correct key(s) on the keyboard, and programming and executing the relevant motor codes. Difficulty at any of these levels of processing will inhibit processing lower levels of activation which results in typing disfluencies. Further, the duration of the delay might, to some extent, depend on the level of activation that is causing the lag. Thresholds are insufficient to distinguish fluent from disfluent typing to force the typing process into a single normal distribution. Crutially important information about the mental process would be removed. Planning the next keystroke may in some cases be more demanding, in particular when planning on phrase boundaries or due to the retrieval of the lexical form of a low frequent word. Also, long values are more likely for struggling writers. For incorrectly produced keystroke intervals, durations may or may not change depending on whether the error was noticed by the participant or create a dysfluency for a subsequent keystroke interval. Importantly, long keystrokes are not necessarily outliers and pauses are a central part of the writing process. 


# Modelling typing process data


As a guiding principle, we aim to produce statistical models that are representative for the theoretical process that creates the data at hand. All models aim to model the typing process based on unaggregated data, i.e. the lags between subseuqent keypresses; e.g. c$^{\wedge}$a$^{\wedge}$t for typing the word _cat_, where the caret $^{\wedge}$ indicates the difference betwee the time for pressing $<$c$>$ and $<$a$>$. Further, our models aim to provide a systematic way of addressing process disfluencies rather than requiring data removal or pausing thresholds. For example when the time for pressing $<$a$>$ after $<$c$>$ is unusually large, our models should make notice of this disfluency. 

We implement statistical models that aim to capture these keystroke interval data. Statistcal models allow us to characterise an underlying data generating process using parameters. For example if we assume that data com from a normal distribution, we can charactersie this distribution with a value $\mu$ that indicates the centre of the distribution and a scale $\sigma^2$ (i.e. the variance); this can be expressed as $y \sim Normal(\mu, \sigma^2)$. We will assume thoughout that the inter-keystroke interval (IKI) data can be characterised with a log-normal distribution, because IKIs are zero-bound and therefore right-skewed. Linear mixed effects models (LMM), as in equation \ref{eq:lmm}, are extensions of this logic which has been used widely for modelling keystroke data [@quene2004multi; @waes2019; @van2010reading]. Crutially, the parameters of interest can be estimated while accounting for variance that is unknown and random. For example, some participants are faster typists than others. The variance associated with the $i$s participant, expressed as $u_i$, can be assumed to be normal distributed around 0 with a between participants variance $\sigma_u^2$ with $i = 1, \dots, I$, where $I$ is the number of participants. After addressing individual typing-speed, the process that generates IKIs can be described by a mean $\mu$ and an error variance $\sigma_e^2$. For model fit purposes, the mean $\mu$ was estimated using a non-centred parametrisation with weakly informative priors [@lambert2018student]. For the error variance we used a standard non-informative half-Cauchy prior [@gelman2014] as the variance must be larger than zero. Generally, we use weakly informative priors to aid model convergence. These priors help to reduce the parameter space to relatively plausible calues. All models presented below share these properties. The following models differ as to how (1) timecourse differences and (2) disfluencies are addressed by these models. 

A standard approach to model the variation between individual keystroke pairs (i.e. bigrams) is to assume that each bigram $j$ with $j = 1, \dots, J$, where $I$ is the total number of bigrams, is independent of the other bigrams and associated with a unknown variance $w_j$ that is distributed around 0 with a between bigram variance $\sigma_w^2$ [see @van2019multilingual]. 

$$
\tag{1}
\begin{aligned}
y_{ij} \sim LogNormal(\mu + u_i + w_j, \sigma_e^2)\\
\text{where}\\
\mu = \mu_{\alpha} + \mu_{\sigma} * \mu_{\eta}\\
\mu_{\alpha} \sim Normal(5,4)\\
\mu_{\sigma} \sim Normal(0,10)\\
\mu_{\eta} \sim Normal(0,1)\\
u_i \sim Normal(0, \sigma_u^2)\\
w_j \sim Normal(0, \sigma_w^2)\\
\sigma_u, \sigma_w \sim Normal(0,2.5)\\
\sigma_e \sim Cauchy(0,2.5)\\
\text{constraint: }\mu_{\sigma}, \sigma_u, \sigma_w, \sigma_e >0 
\end{aligned}
(\#eq:lmm)
$$

Further, we can extent this model by assuming that larger variations in typing differences for bigrams depend on the typing speed of each participant. For example, fast participants might show less variation between bigrams that slow participants. This assumption can be modelled by including by-participant slope adjustments for bigrams by introducing a variance-covariance matrix $\Sigma_u$; LKJ prior with $\nu=2.0$ [@lewandowski2009generating]. 



## Typing as autoregressive process

The standard analysis can model variation that is associated with particular bigrams but does allow us to address disfluencies in the typing process beyond assuming they are subject to random statistical noise. Further, the standard analysis assumes that subsequent keystrokes are independent and thus exchangable. If we assume that subsequent keystrokes are autocorrelated [@eltahir2004dynamic], a disfluency can be characterised as a slowdown relative to the previous keystroke interval. In other words, we can predict IKIs with the previous keystroke interval. This feature is at the heart of autoregressive models. These models capture the autocorrelation between subsequent keystrokes by introducing a parameter $\phi$ that predicts an IKI $y_j$ on the basis of the IKI preceding it $y_{j-1}$; see equation \ref{eq:ark}. $\phi_i$ was estimated separately for each participant with a pooled mean $\phi$ and error variance $\eta^2$. Note, to allow comparisons between this and all other models we had exclude the first bigram for each participant, i.e. $y_{ij+1}$.


$$
\tag{2}
\begin{aligned}
y_{ij} \sim LogNormal(\mu + \phi_i*log(y_{ij-1}) + u_i, \sigma_e^2)\\
\text{where}\\
\mu = \mu_{\alpha} + \mu_{\sigma} * \mu_{\eta}\\
\phi_i \sim Normal(\phi, \eta^2)\\
\mu_{\alpha} \sim Normal(5,4)\\
\mu_{\sigma} \sim Normal(0,10)\\
\mu_{\eta}, \phi \sim Normal(0, 1)\\
\eta \sim Cauchy(0, 1)\\
u_i \sim Normal(0, \sigma_u^2)\\
\sigma_u \sim Normal(0,2.5)\\
\sigma_e \sim Cauchy(0,2.5)\\
\text{constraint: }\mu_{\sigma}, \sigma_u, \sigma_e, \eta >0 
\end{aligned}
(\#eq:ark)
$$


## Typing as (autoregressive) finite mixture process

Writing data are generally speaking a combination of fluent typing and typing disfluencies. While disfluencies can be subject to processing on various levels of mental representations they will always be expressed as longer keystroke intervals. To capture this mixture of fluent and disfluent typing, we implemented a finite mixture model. Mixture models assume that data come from a combination of distributions. For the present purpose we constrain the model to be finite. In other words, we fixed the number of underlying distributions to two, namely, 2 log-Gaussian (normal) distributions of which on represents fluent tying (shorter keystroke intervals) and the other process disfluencies (longer keystroke intervals). This model can be summarised as in equation \ref{eq:mog}, following @vasishth2017. Both distributions have the same mean $\mu$ but for the first the parameter $\delta$ that was constrainted to be positive was added. The mixing proportion $\theta_i$, then, captures the probability of a data point to be part of either distribution (fluent or disfluent typing) for each participant $i$ which a mean $\theta$ and variance $\tau^2$. The mixing proportion $\theta_i$ was transformed to range from 0 to 1 (inverse logit) where a value of 0 would indicate fluent typing and 1 indicates disfluency. As longer latencies are known to be associated with a larger variances for both response-time data in particular [@wagenmakers2007linear] and human motor bahaviour in general [@wing1973response;@schoner2002timing], the variance $\sigma_{e'}^2$ associated with the distribution of typing disfluencies was constrained to be larger than the variance for normal typing $\sigma_e^2$ . 


$$
\tag{3}
\begin{aligned}
	y_{ij} \sim \theta_i \cdot LogNormal(\mu + \delta + u_i + w_j, \sigma_{e'}^2) +\\
		(1 - \theta_i) \cdot LogNormal(\mu + u_i + w_j, \sigma_{e}^2)\\
		\text{where}\\
		\theta_i = InvLogit(\theta_i)\\
		\mu = \mu_{\alpha} + \mu_{\sigma} * \mu_{\eta}\\
		\sigma_{e'} = \sigma + \sigma_{\text{diff}}\\
		\sigma_{e} = \sigma - \sigma_{\text{diff}}\\
		\theta_i \sim Normal(\theta,\tau^2)\\
    \mu_{\alpha} \sim Normal(5,4)\\
    \mu_{\sigma} \sim Normal(0,10)\\
		\theta, \delta, \mu_{\eta}, \sigma_{\text{diff}} \sim Normal(0,1)\\
		\tau \sim Cauchy(0,1)\\
		u_i \sim Normal(0, \sigma_u^2)\\
    w_j \sim Normal(0, \sigma_w^2)\\
    \sigma_u, \sigma_w \sim Normal(0,2.5)\\
		\sigma \sim Cauchy(0,2.5)\\
		\text{constraint: } \mu_{\sigma}, \sigma_u, \sigma_w, \delta, \tau, \sigma, \sigma_{\text{diff}}, \sigma_{e'}, \sigma_{e} > 0
\end{aligned}	
(\#eq:mog)
$$

Note that the former model implies that subsequent keystroke intervals are independent. This might be the case for disfluencies but fluent typing might involve autocorrelations. Therefore, we implemented another mixture model but replaced the bigram intercepts $w_j$ in the previous model with an autoregressor $\phi_i*y_{ij-1}$ in the distribution that represents fluent typing, as in equation \ref{eq:ark}, but included random bigram intercepts for the distribution of disfluent typing intervals. 


```{r }
# Load df
path <- "../data/"
d <- get_data(path = path) %>% filter(component == "Consonants") %>% select(-component)
d.ppt <- d %>% select(subj, age, sex) %>% unique()
d.sex <- d.ppt %>% count(sex) %>% pull(n) # femanle, male
d.age <- d.ppt %>% summarise(M = median(age), min = min(age), max = max(age)) %>%  gather(p, value) %>% pull(value) %>% round(0) 
```


# Method

We evaluate our models using data from a subset of the Dutch copy-task corpus [@leijten2013keystroke;@waes2019;@van2019multilingual]. The copy-task corpus consists of keystroke data collected in Inputlog, a Javascript-based web application, available on \url{www.inputlog.net}. In this analysis we focus on the consonant task because it is a demanding but short task that allows us to measure typing skills in a non-linguistic environment [@grabowski2010second]. The consonant task is relatively short but complex in the sense that participants have to look at the target string repeatedly to copy it accurately. In other words, for this task we need to be able to disentagle fluent and disfluent latencies. Participants saw and copy-typed four blocks of six consonant sequences "tjxgfl pgkfkq dtdrgt npwdvf" that do not occur adjacently in Dutch. The dependent variable are the inter-keystroke intervals (IKI) between two consecutive consonants (key presses). Spaces were excluded from the data. From this corpus we took a random sample of `r nrow(d.ppt)` participants (`r d.sex[1]` female, `r d.sex[2]` male) in the age range between `r d.age[2]` and `r d.age[3]` years (median age = `r d.age[1]` years). We used decided this age range to keep the sample relatively homogeneous.


# Results

## Data overview

The data are visualized in Figure \ref{fig:descriptives}. The figure shows the variations of IKIs throughout the copy-typing process. Each grey line represents one participant and the coloured lines shown different measures of central tendency. The central tendency descriptives represent different properties of the distribution as can be seen in the density plot in the right margin. The mean is sensitive to larger IKIs and is therefore in the centre of the distribution while the mode and the median are closer to the most frequent value. All central tendencies vary depending on the position of the bigram in the string. The up-and-down pattern in the data suggests that IKIs are sensitive to the position of a bigram in a chunk rather than bigram identity or the position of the bigram in the entire sequence.


```{r descriptives, fig.pos="!ht", fig.height = 3, fig.width = 6, fig.align = "center", fig.cap="IKI data by participant over bigrams position shown in order. Shown are the raw data, each line representing one participant, and different measures of central tendency (with 2$\\times$standard error [SE]). Vertical lines indicate the position of SPACES. The density distribution of IKI data is shown in the right margin."}

Ms <- c("Median" = "dotted", "Mean" = "dashed", "Mode" = "longdash")
Mcol <- c("Median" = "darkolivegreen4", "Mean" = "turquoise4", "Mode" = "darkred")

d %<>% select(subj, bg, bigram, IKI)

d %>% group_by(subj) %>%
  mutate(bigram = 1:n()) %>%
  arrange(subj, bigram) %>%
  ungroup() %>%
  select(subj, bigram, bg) %>%
  pivot_wider(names_from = bigram, values_from = bg) %>% 
  unite("sequence", `1`:`20`, na.rm = T, remove = F, sep = " ") %>%  
  ungroup() %>%
  mutate(sequence = trimws(sequence)) %>%
  group_by(sequence) %>%
  mutate(n = n()) %>%
  mutate(p = n/500*100) -> d_wide

d %>% left_join(d_wide) %>% select(-n) %>% rename(p_seq = p) -> d2

d2 %>% ungroup() %>% group_by(subj) %>%
  mutate(bigram = 1:n()) %>%
  ungroup() %>%
  group_by(bigram) %>%
  mutate(N = n()) %>%
  filter(bigram < 21) %>% 
  summarise(Mode = dmode(IKI),
            Mean = mean(IKI),
            Median = median(IKI),
            SE = 2*sd(IKI)/sqrt(n())) %>% 
  ungroup() -> d_summary

#tjxgfl pgkfkq dtdrgt npwdvf
d_summary$bgs <- c("tj", "jx", "xg", "gf", "fl",
                   "pg","gk", "kf", "fk",  "kq", 
                   "dt", "td", "dr", "rg", "gt", 
                   "np", "pw", "wd", "dv", "vf")

d_summary %<>%
  mutate(bgs = factor(bgs, levels = unique(bgs)[bigram], ordered = T))

d2 %>% ungroup() %>% group_by(subj) %>% mutate(bigram = 1:n()) %>% 
  filter(bigram < 21) %>% ungroup() %>%
  left_join(d_summary[,c("bigram", "bgs")]) %>%
  mutate(bgs = factor(bgs, levels = unique(bgs)[unique(bigram)], ordered = T)) %>% ggplot() +
  geom_point(aes(y = IKI, x = bgs, linetpye = factor(subj), group = factor(subj)),
             size =.5, show.legend = F, alpha = .1) +
  geom_line(aes(y = IKI, x = bgs, linetpye = "dashed", 
                group = factor(subj)), show.legend = F, alpha = .05) +
  geom_pointrange(data = d_summary, 
                  aes(y = Mode, x = bigram, ymin = Mode - SE, ymax= Mode + SE, color = "Mode"),
                  fatten = 3) +
  geom_pointrange(data = d_summary, 
                  aes(y = Mean, x = bigram, ymin = Mean - SE, ymax= Mean + SE, color = "Mean"),
                  fatten = 3) +
  geom_pointrange(data = d_summary, 
                  aes(y = Median, x = bigram, ymin = Median - SE, ymax= Median + SE, color = "Median"),
                  fatten = 3) +
  geom_line(data = d_summary, aes(y = Mode, x = bgs, group = 1, color = "Mode", linetype = "Mode")) +
  geom_line(data = d_summary, aes(y = Median, x = bgs, group = 1, color = "Median", linetype = "Median")) +
  geom_line(data = d_summary, aes(y = Mean, x = bgs, group = 1, color = "Mean", linetype = "Mean")) +
  geom_vline(data = filter(d_summary),
             aes(xintercept = c(5.5)), linetype = "dotted", size = .25) +
  geom_vline(data = filter(d_summary),
             aes(xintercept = c(10.5)), linetype = "dotted", size = .25) +
  geom_vline(data = filter(d_summary),
             aes(xintercept = c(15.5)), linetype = "dotted", size = .25) +
  theme_few(base_size = 10) + scale_y_log10() +
  labs(x = "Bigram (in order)", y = "log-scaled IKIs [in msecs]") +
  scale_linetype_manual(name = "", values = Ms) +
  scale_color_manual(name = "", values = Mcol) +
  theme(strip.text = element_text(size = 12, hjust=0),
        legend.position = "none",
        plot.margin = unit(c(.25, .1, .25, .1), "cm"),
                axis.ticks = element_blank()) -> p_big

d.raw <- d2 %>% ungroup() %>% 
  group_by(subj) %>% mutate(bigram = 1:n()) %>% 
  filter(bigram < 21) %>% ungroup() %>% summarise(mean = mean(IKI), 
                         median = median(IKI), 
                         mode = dmode(IKI))


d2 %>% ungroup() %>% group_by(subj) %>% mutate(bigram = 1:n()) %>% filter(bigram < 21) %>% ungroup() %>%
  ggplot(aes( x = IKI)) +
  stat_density(size = .2, color = "black", fill = "white") +
  scale_x_log10() +
  theme_void(base_size = 10) +
  labs(x = "", y = "") +
  geom_segment(data=d.raw, aes(x=mean, y=0, yend = .75, xend = mean, linetype="Mean", color = "Mean")) +
  geom_segment(data=d.raw, aes(x=median, y=0, yend = 1.1, xend = median, linetype="Median", color = "Median")) +
  geom_segment(data=d.raw, aes(x=mode, y=0, yend = 1.25, xend = mode, linetype="Mode", color = "Mode")) +
  theme(axis.text = element_blank(),
        axis.ticks = element_blank(),
        legend.position = c(.7,1),
        legend.key.width = unit(1, "cm"),
        legend.justification = "top",
        plot.margin = unit(c(.25, 1, .25, -.15), "cm")) +
  scale_linetype_manual(name = "", values = Ms) +
  scale_color_manual(name = "", values = Mcol) +
  coord_flip() -> p.log


cowplot::plot_grid(p_big, p.log,  ncol = 2, align = "h",
      rel_widths = c(3, 1))

```



## Model comparisons

All models were implemented as Bayesian models [see e.g. @gelman2014; @lambert2018student; @mcelreath2016statistical] in the probabilistic programming language Stan [@carpenter2016stan; @rstan; @rstan2; @hoffman2014no]. \textit{R} and \textit{Stan} code are available on [GitHub](https://github.com/jensroes/Frontline) for reproducibility. Models were fitted with weakly regulating priors and run with 30,000 iterations (15,000 warm-up) on 3 Markov chain Monte Carlo chains. Convergence was tested via the Rubin-Gelman statistic [@gelman1992], traceplots and cross-validation [@vehtari2015pareto; @vehtari2017practical].

The predictive performance of the models was established using leave-one-out cross-validation which penalizes models with more parameters [see @farrell2018computational; @mcelreath2016statistical; @lambert2018student; @lee2014bayesian]. The out-of-sample predictive performance was determined via Pareto smoothed importance-sampling [@vehtari2015pareto; @vehtari2017practical] and estimated as sum of the expected log predictive density ($\widehat{elpd}$). $\widehat{elpd}$ was used to compare the predictive quality of our models. Model comparisons can be found in Table \ref{tab:modelcomparisons}. The best fitting model was revealed to be the mixture model M4 (see equation \ref{eq:mog}).

```{r modelcomparisons, results = "asis"}
looc <- read_csv("../results/loo_results_consonants.csv") %>%
  mutate_if(is.numeric, round, 0) %>%
  unite(col = "elpd_diff", elpd_diff, se_diff, sep = " (") %>%
  unite(col = "elpd_loo", elpd_loo, se_elpd_loo, sep = " (") %>%
  mutate(elpd_diff = paste0(elpd_diff, ")")) %>%
  mutate(elpd_loo = paste0(elpd_loo, ")")) %>%
  mutate(Model = recode(Model, ARK1ppt = "M3",
                               LMMbigramintercepts = "M1",
                               LMMbigramslopes = "M2",
                               MoGpptsbigramintercepts = "M4",
                               ARKMoGppt = "M5")) %>%
  mutate(Type = recode(Model, M2 = "LMM",
                              M1 = "LMM",
                              M3 = "AR",
                              M4 = "MoG",
                              M5 = "AR + MoG")) %>%
  mutate(Bigrams = recode(Model, M3 = "Autoregressor $\\phi$",
                              M2 = "Random intercepts and slopes",
                              M1 = "Random intercepts",
                              M4 = "Random intercepts",
                              M5 = "Autoregressor $\\phi$ and random intercepts")) %>%

  select(Model, Type, elpd_diff, elpd_loo, Bigrams)

names(looc)[c(3,4)] <- c("$\\Delta\\widehat{elpd}$", "$\\widehat{elpd}$")

papaja::apa_table(looc,
                  align =  "llrrl",#c("l", "l", "l", "r", "r"), 
                  escape = FALSE, 
                  placement = "!ht",
                  caption = "Model comparisons $\\Delta\\widehat{elpd}$ expressed as expected log predictive density ($\\widehat{elpd}$) with SE in parentheses. The bigrams column indicates how the models account for by-bigram variation. The top row shows the model with the highest predictive performance.",  
                  note = "LMM = linear mixed effects models; AR = Autoregressive model; MoG = Mixture of (log-)Gaussians"
) 

mc <- read_csv("../results/loo_results_consonants_mog.csv") %>% filter(Model == "MoGpptsbigramintercepts") %>% mutate_if(is.numeric, round, 0)
```


The second best performing model is the mixture model with the autoregressor $phi$ for fluent typing. In other words, adding the autoregressor instead of random bigram intercepts for fluent typing did not improve the predictive performance of the model. In fact, the autoregressive model was found to be the model with the lowest predictive performance among the models tested. Modelling bigrams as random intercepts (with and without by-participant slope adjustements) was found to have a higher predictive performance compared to the autoregession model [@van2010reading;@waes2019]. Overall the best fit among the tested models was revealed by the mixture model with a distribution for fluent typing and a distribution for disfluencies while accounting for bigrams as random variance.




```{r}
ps <- read_csv("../results/consonants_posterior_MoG.csv")

ps %>% select(starts_with("prob")) %>%
  gather(Param, value) %>%
  group_by(Param) %>%
  summarise(M = mean(value),
            lo = quantile(value, probs = .025),
            up = quantile(value, probs = .975),
            p = mean(value < .5)) %>%
  ungroup() %>%
  separate(Param, into = c("Param", "id"), sep = "\\[") %>%
  mutate(id = gsub("\\]", "", id),
         id = ifelse(is.na(id), 0, id)) %>%
  arrange(M) %>%
  mutate(id2 = ifelse(id == 0, 1, 0),
         id2 = 1:n(),
         id2 = factor(id2, levels = unique(id2)[id2], ordered = T)) -> d_theta

d_theta %>% filter(id == 0) %>% pivot_longer(M:p) %>% pull(value) -> theta_sum

ps %>% select(starts_with("beta")) %>%
  select(-starts_with("beta2")) %>%
  gather(Param, value) %>%
  mutate(value = exp(value)) %>%
  group_by(Param) %>%
  summarise(M = mean(value),
            lo = quantile(value, probs = .025),
            up = quantile(value, probs = .975)) %>%
  ungroup() %>%
  separate(Param, into = c("Param", "id"), sep = "\\[") %>%
  mutate(id = gsub("\\]", "", id),
         id = ifelse(is.na(id), 0, id)) %>%
  arrange(M) %>%
  mutate(id2 = ifelse(id == 0, 1, 0),
         id2 = 1:n(),
         id2 = factor(id2, levels = unique(id2)[id2], ordered = T)) -> d_beta

d_beta %>% filter(id == 0) %>% pivot_longer(M:up) %>% pull(value) -> beta_sum

ps %>% select(starts_with("prob_s"), starts_with("beta_s")) %>%
  mutate_at(vars(starts_with("beta_s")), exp) %>%
  pivot_longer(everything()) %>%
  separate(name, into = c("param", "subj"), sep = "_") %>%
  group_by(param, subj) %>%
  mutate(rep = 1:n()) %>%
  group_by(subj, param) %>%
  summarise(M = mean(value),
            lo = quantile(value, probs = .025),
            up = quantile(value, probs = .975)) %>%
  pivot_wider(names_from = param, values_from = c(M, lo, up)) -> pss


ps %>% select(beta, delta) %>%
  transmute(delta = exp(beta+delta) - exp(beta)) %>%
  summarise(mean(delta), quantile(delta, probs = .025), quantile(delta, probs = .975)) %>%
  pivot_longer(everything()) %>% pull(value) %>% round(0) -> delta_sum

```


## Model evaluation

The copy-typing process captured by the mixture model can be assessed using the posterior distributions of the parameter values. The process relevant model parameter values are illustrated in Figure \ref{fig:parameters}. Firstly, IKIs, excluding disfluencies, are shown by participant in Figure \ref{fig:parameters}A. The red line indicates the pooled overall parameter estimate for fluent typing $\hat{\beta}$=`r round(beta_sum[1],0)` msecs centred around a 95% PI of [`r round(beta_sum[2],0)`, `r round(beta_sum[3],0)`]. For each participant the probability of disfluent typing is shown in Figure \ref{fig:parameters}B. The overall disfluency probability (in red) was $\hat{\theta}$=`r round(theta_sum[1],2)` centred around 95% PI[`r round(theta_sum[2],2)`, `r round(theta_sum[3],2)`]. In other words, for the consonant task we observe `r round(theta_sum[1],2)*100`\% disfluent typing, and hence `r round(1-theta_sum[1],2)*100`\% fluent typing. 

The y-axis in panel A and B are ordered by the average size of the respective values, thus the lines do not represent the same participants. In fact, Figure \ref{fig:parameters}C suggests that the inferred latency for fluent typing and the probability to exhibit dislfuences are independent. Shown are the parameter estimates for each participant and the overall pooled estimates. In other words, the frequency of process disfluencies does not depend on the average typing speed. Fast as well as slow copy-typists can show low and high disfluency probabilities. Finally, the slowdown for disfluent typing is shown in Figure \ref{fig:parameters}D. Disfluent typing in the consonent task is $\hat{\delta}$=`r delta_sum[1]` msecs (95% PI[`r delta_sum[2]`, `r delta_sum[3]`]) slower than fluent typing.^[The size of the disfluency $\delta$ could in principle vary by participant. For example, for faster participants might show larger disfluencies and slower participants might show smaller disfluencies. We addressed this possibility by allowing the disfluency parameter $\delta$ to vary for each participant. This model reavealed a negligible gain over model M4 ($\Delta\widehat{elpd}=$`r mc$elpd_diff`, SE=`r mc$se_diff`).] Overall, there is a large probability of dysfluent typing in the consonant task. This makes it crutial to distinguish between keystroke intervals that represent fluent and disfluent typing. Mixture models can provide accurate estimates for fluent typing by accounting disfluencies in a principled matter; i.e. be modelling fluent and disfluent typing as a mixture process.


```{r parameters, fig.pos="!ht", fig.height=6, fig.width=6, fig.align = "center", fig.cap="Posterior parameter values of the mixture model. Panel A shows by-participant IKIs and overall IKI value for fluent typing in red. Panel B shows by-participant disfluency probability (overall parameter value $\\theta$ in red). Panel C shows fluent IKIs plotted against disfluency probability (red triangle indicates overall parameter value). Panel D shows the posterior distribution of the disfluency slowdown. All error bars are 95\\% probability intervals."}

ggplot(d_beta, aes(y = M, ymin = lo, ymax =up, x =id2, 
                   color = Param, 
                   shape = Param,
                   size = Param)) +
  geom_hline(yintercept = beta_sum[1], linetype = "dotted") +
  geom_errorbar(alpha = .5, width = 0, size = .35, show.legend = F) +
  geom_point(show.legend = F) +
  scale_color_manual(values = c("darkred", "black")) +
  scale_shape_manual(values = c(17,19)) +
  scale_size_manual(values = c(1, .1)) +
  coord_flip() + theme_few(base_size = 10) +
  labs(x = "Participants", 
       y = bquote(atop("IKIs"~hat(beta)~"[in msecs]"))) +
  theme(axis.text.y = element_blank(),
        axis.ticks = element_blank()) -> p_beta

ggplot(d_theta, aes(y = M, ymin = lo, ymax =up, x =id2, 
                    color = Param, shape = Param, size = Param)) +
  geom_hline(yintercept = theta_sum[1], linetype = "dotted") +
  geom_errorbar(alpha = .5, width = 0, size = .35, show.legend = F) +
  geom_point(show.legend = F) +
  scale_color_manual(values = c("darkred", "black")) +
  scale_shape_manual(values = c(17,19)) +
  scale_size_manual(values = c(1, .1)) +
  coord_flip() + theme_few(base_size = 10) +
  labs(x = "Participants", 
       y = bquote(atop("Disfluency probability"~hat(theta)))) +
  theme(axis.text.y = element_blank(),
        axis.ticks = element_blank()) -> p_theta


ggplot(pss, aes(x = M_beta, y = M_prob)) +
  geom_point(size = .5, alpha = .8) +
  geom_errorbar(aes(ymin = lo_prob, ymax = up_prob), show.legend = F, size =.05, alpha =.15) +
  geom_errorbarh(aes(xmin = lo_beta, xmax = up_beta), show.legend = F, size = .05, alpha =.15) +
  geom_vline(xintercept=beta_sum[1], linetype="dashed", alpha = 0.4, colour = "darkred") +
  geom_hline(yintercept=theta_sum[1], linetype="dashed", alpha = 0.4, colour = "darkred") +
  geom_errorbar(inherit.aes = F, aes(ymin = theta_sum[2], ymax = theta_sum[3], x = beta_sum[1]), 
                  colour = "darkred", size =.1, width = 0) +
  geom_errorbarh(inherit.aes = F, aes(xmin = beta_sum[2], xmax = beta_sum[3], y = theta_sum[1]), 
                  colour = "darkred", size =.1, height = 0) +
  geom_point(inherit.aes =F, aes(x = beta_sum[1], y = theta_sum[1]), shape=17, colour = "darkred")+
  theme_few(base_size = 10) +
  labs(y = bquote("Disfluency probability"~hat(theta)),
       x = bquote("IKIs"~hat(beta)~"[in msecs]")) +
  theme(axis.ticks = element_blank()) -> p_scatter

ps %>% select(beta, delta) %>%
  transmute(delta = exp(beta+delta) - exp(beta)) %>%
  ggplot(aes(x = delta)) +
  geom_density(fill = "darkred", alpha = .45, colour = "darkred") +
  geom_segment(aes(x = quantile(delta, probs = .025), 
                   xend = quantile(delta, probs = .975), y=0, yend=0), size=1) +
  geom_point(aes(x = mean(delta), y =0), size = 2, color = "darkred", shape = 17) +
  labs(x = bquote("Disfluency slowdown"~hat(delta)~"[in msecs]"), 
       y = "Posterior density") +
  theme_few(base_size = 10) +
  theme(axis.ticks = element_blank(),
        axis.text.y = element_blank()) -> p_param

cowplot::plot_grid(p_beta, p_theta, p_scatter, p_param, align = "vh",
          labels = c("A", "B", "C", "D"), ncol=2, nrow = 2)
```




```{r}
#ytilde <- read_csv("../results/consonants_posterior_MoG_ytilde.csv")
#y <- d %>% filter(bigram != 1)
#ytilde %<>%
#  mutate(samples = 1:n()) %>%
#  filter(samples %in% sample(samples, 500)) %>%
#  pivot_longer(cols = starts_with("y_tilde"), names_to = "id", values_to = "y_tilde") 
#ggplot() +
#  geom_density(data = ytilde, aes(x = y_tilde, group = samples, linetype = "y_tilde"), 
#               alpha = .25, colour = "darkred") + 
#  geom_density(data= y, aes(x = IKI, linetype = "y"), fill = "darkred", alpha = .25) +
#  scale_x_log10() +
#  scale_linetype_manual(values = c("y_tilde" = "dotted", "y" = "solid"))
```




# Discussion

Our aim was provide a statistical model of inter-keystroke intervals that addresses process disfluencies in a principled manner. We compared series of Bayesian models addressing this aim. Model comparisons showed that, among our models, process disfluencies can be captured best as a mixture process. This model captures disfluencies in the typing process by assuming that typing data come from a combination of two distributions, representing fluent and disfluent typing, of which each has a latent probability. This model allows us to extract reliable typing-interval estimates for fluent typing while accounting for process difluencies in a principled manner. 

This model provides a) overall and by-participant keystroke intervals for fluent tpying, b) overall and by-participant disfluency probabilities, and c) an parameter estimate that captures the overall slowdown for disfluent trials. These parameter estimates are relevant on two levels. First, they allows us to characterise the writing task at hand. For example, we obseve that copy-typing non-lexical strings of consonants is ridden with disfluencies. Second, by-participant parameter estimates all us to extract typing characteristics for individual typists. In particular, we extracted each participants fluent typing speed and the probability of disfluencies exhibited by each participant. Taken both, overall and individual parameter estimates together, we can determine whether an individual typist was a fast / slow typist or had unusually high / low probability to exhibit disfluencies. 


The central advantage of using mixture models to account for typing disfluencies is that we can by-pass the use of treshold values to define disfluencies. From the raw data it is not possible to know which data are disfluencies a priori. Using threshold values merely ignores that some participants are generally lower typists and some tasks are easier or more difficult. Mixture models allow us to capture disfluencies as a latent process in a principled way. This is important because the mixture model presented in this paper takes into account that a disfluency is relative to an individuals tpying speed. Therefore, this would allow to test predictions about typing disfluencies in certain population such as very young or old typists, language learners or individuals with genuine typing difficulty after account for individual differences in typing speed. Further while we used a relatively difficult copy-task to test our models, other writing tasks are bound to lead to other disfluency estimates. In fact, the presented model can be used to test hypothesis about psychological factors that might lead to varying proportions of disfluencies, and thefore changes in cognitive demands, in the writing process. As an avenue for future research, mixture models as presented in this paper can be used for different types of writing tasks and particular populations.

Writing involves processing on various cognitive levels, from generating an idea, retriving lexical entries to the encoding of spelling. As activation between these levels cascades from higher to lower levels of representation, a delay at any of these levels will cause disfluencies. In other words, disfluencies are not just an interesting but a central part of the writing process. Mixture models allows us to address the assumed combination of processes that underlies writing by mapping these of a combination of distributions. Using a combination of two distributions is merely reducing this combination into normal, fluent, typing and typing that involves processing delays on higher levels (i.e. disfluencies). If one assumes that the size of the disfluency depends on the inhibited upstream process, one may implement this as an additional mixture component [see e.g. @baaijen2012keystroke] for address different types of disfluencies [@wengelin2001disfluencies].






# References
```{r create_r-references, echo=FALSE, include=FALSE}
r_refs(file = "ref.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
  
<div id = "ref"></div>
\endgroup
  


